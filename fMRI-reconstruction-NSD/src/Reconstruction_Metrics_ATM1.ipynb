{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439f938-67e1-405a-913a-4f4fd487aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to convert this notebook to .py if you want to run it via command line or with Slurm\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Reconstruction_Metrics.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53146de1-97c4-4bf9-9569-a6a400d377fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "# from models import Clipper\n",
    "# clip_extractor = Clipper(\"ViT-L/14\", hidden_state=False, norm_embs=True, device=device)\n",
    "imsize = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72944c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "istest = True\n",
    "sub = 4\n",
    "# Define the source and target directories\n",
    "if istest:  \n",
    "    source_dir = '../../Generation/generated_imgs'+'/'+str(sub)\n",
    "else:\n",
    "    source_dir = '../../Generation/val_generated_imgs'+'/'+str(sub)\n",
    "target_dir = '../../Generation/generated_imgs_tensor'\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Initialize a list to hold all the image tensors\n",
    "tensor_list = []\n",
    "# Initialize a dictionary to map image file names to their categories\n",
    "image_categories = {}\n",
    "\n",
    "# Initialize the set to store unique category names\n",
    "category_set = set()\n",
    "\n",
    "# Iterate over the folders in the source directory\n",
    "for folder_name in sorted(os.listdir(source_dir)):\n",
    "    folder_path = os.path.join(source_dir, folder_name)\n",
    "\n",
    "    # Extract the category name from the folder name (assuming category is part of folder_name)\n",
    "    category_name = folder_name.split(' ')[-1]  # This splits the folder_name and takes the last part as the category\n",
    "    category_set.add(category_name)  # Add the category name to the set\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Sort the image files to ensure consistent order\n",
    "        image_files = sorted(os.listdir(folder_path))\n",
    "        # Iterate over the sorted images in the folder\n",
    "        for image_name in image_files:\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "            # Map the image name to its category\n",
    "            image_categories[image_name] = category_name\n",
    "\n",
    "            # Load the image\n",
    "            with Image.open(image_path) as img:\n",
    "                # Convert the image to a PyTorch tensor and add a batch dimension\n",
    "                tensor = torch.tensor(np.array(img)).unsqueeze(0)\n",
    "                tensor_list.append(tensor)\n",
    "\n",
    "# Concatenate all tensors along the 0th dimension\n",
    "all_tensors = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "# Save the combined tensor\n",
    "combined_tensor_path = os.path.join(target_dir, \"all_images.pt\")\n",
    "torch.save(all_tensors, combined_tensor_path)\n",
    "\n",
    "# Now we sort the category names and write them to a file\n",
    "# along with their associated image names\n",
    "categories_path = os.path.join(target_dir, \"categories.txt\")\n",
    "with open(categories_path, 'w') as f:\n",
    "    for image_name, category_name in sorted(image_categories.items()):\n",
    "        f.write(f\"{image_name}: {category_name}\\n\")\n",
    "\n",
    "# Print out the category list, now sorted\n",
    "category_list = sorted(list(category_set))\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc126d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "splits_path = \"../../../datasets/block_splits_by_image_all.pth\"#++\n",
    "# splits_path = \"../../../datasets/block_splits_by_image_single.pth\"#++\n",
    "eeg_signals_path=\"../../../datasets/eeg_5_95_std.pth\"#++.\n",
    "# eeg_signals_path=\"../../../datasets/eeg_signals_raw_with_mean_std.pth\"#++.\n",
    "loaded = torch.load(eeg_signals_path)\n",
    "images = loaded[\"images\"]\n",
    "data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==sub]\n",
    "if istest:\n",
    "    split_idx = torch.load(splits_path)[\"splits\"][0]['test']\n",
    "else:\n",
    "    split_idx = torch.load(splits_path)[\"splits\"][0]['val']\n",
    "split_idx = [i for i in split_idx if i < len(data) and 450 <= data[i][\"eeg\"].size(1) <= 600]\n",
    "# for i in split_idx:\n",
    "#     print(i)\n",
    "images = [images[data[i][\"image\"]] for i in split_idx]\n",
    "image_paths = [os.path.join('../../../datasets/imageNet_images/', image.split('_')[0], image+'.JPEG') for image in images]\n",
    "# Define the source and target directories\n",
    "\n",
    "if istest:\n",
    "    target_dir = '../../../datasets/test_images_tensor'\n",
    "else:\n",
    "    target_dir = '../../../datasets/val_images_tensor'\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Initialize a list to hold all the image tensors\n",
    "tensor_list = []\n",
    "# Initialize a dictionary to map image file names to their categories\n",
    "image_categories = {}\n",
    "\n",
    "# Initialize the set to store unique category names\n",
    "category_set = set()\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "])\n",
    "for image_path in sorted(image_paths):\n",
    "    category_name = image_path.split('/')[-1].split('.')[0]\n",
    "    category_set.add(category_name)\n",
    "    image_name = image_path.split('/')[-1].split('.')[0]\n",
    "    image_categories[image_name] = category_name\n",
    "    with Image.open(image_path).convert('RGB')  as img:\n",
    "        # Convert the image to a PyTorch tensor and add a batch dimension\n",
    "        tensor = img_transform(torch.tensor(np.array(img)).permute(2,0,1)).permute(1,2,0).unsqueeze(0)\n",
    "        tensor_list.append(tensor)\n",
    "\n",
    "tensor = tensor.squeeze(0)\n",
    "\n",
    "# Concatenate all tensors along the 0th dimension\n",
    "all_tensors = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "# Save the combined tensor\n",
    "combined_tensor_path = os.path.join(target_dir, \"all_images.pt\")\n",
    "torch.save(all_tensors, combined_tensor_path)\n",
    "\n",
    "# Now we sort the category names and write them to a file\n",
    "# along with their associated image names\n",
    "categories_path = os.path.join(target_dir, \"categories.txt\")\n",
    "with open(categories_path, 'w') as f:\n",
    "    for image_name, category_name in sorted(image_categories.items()):\n",
    "        f.write(f\"{image_name}: {category_name}\\n\")\n",
    "\n",
    "# Print out the category list, now sorted\n",
    "category_list = sorted(list(category_set))\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf904985-a9ed-4d12-a965-a46db5473d1b",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a4650-57e8-443a-8c09-05b10e013137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if running this interactively, can specify jupyter_args here for argparser to use\n",
    "# if utils.is_interactive():\n",
    "#     # Example use\n",
    "#     jupyter_args = \"--recon_path=prior_257_final_subj01_bimixco_softclip_byol_brain_recons_full_img2img0.85_16samples.pt\"\n",
    "    \n",
    "#     jupyter_args = jupyter_args.split()\n",
    "#     print(jupyter_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef907d0f-b2dc-4035-a212-078da9eb5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "# parser.add_argument(\n",
    "#     \"--recon_path\", type=str,\n",
    "#     help=\"path to reconstructed/retrieved outputs\",\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--all_images_path\", type=str, default=\"all_images.pt\",\n",
    "#     help=\"path to ground truth outputs\",\n",
    "# )\n",
    "\n",
    "# if utils.is_interactive():\n",
    "#     args = parser.parse_args(jupyter_args)\n",
    "# else:\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "# # create global variables without the args prefix\n",
    "# for attribute_name in vars(args).keys():\n",
    "#     globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb10ae2-0188-454a-b227-3a90783bdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_path = '../../Generation/generated_imgs_tensor/all_images.pt'\n",
    "if istest:\n",
    "    all_images_path = '../../../datasets/test_images_tensor/all_images.pt'\n",
    "else:\n",
    "    all_images_path = '../../../datasets/val_images_tensor/all_images.pt'\n",
    "all_brain_recons = torch.load(f'{recon_path}')\n",
    "all_images = torch.load(f'{all_images_path}')\n",
    "all_brain_recons = all_brain_recons[::1]\n",
    "\n",
    "\n",
    "all_images = all_images.to(device)\n",
    "\n",
    "all_images = all_images.transpose(1, 3)\n",
    "all_images = all_images.transpose(2, 3)\n",
    "all_brain_recons = all_brain_recons.transpose(1, 3)\n",
    "all_brain_recons = all_brain_recons.transpose(2, 3)\n",
    "print(all_images.shape)\n",
    "print(all_brain_recons.shape)\n",
    "\n",
    "all_images = all_images.to(device)\n",
    "all_brain_recons = all_brain_recons.to(device)\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 创建保存图像的目录\n",
    "save_path = '../../testandrecon'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# 将张量转换为图像并保存\n",
    "def save_images(tensor, prefix, path):\n",
    "    for i in range(tensor.size(0)):\n",
    "        img = tensor[i].cpu().numpy().transpose(1, 2, 0)  # 将张量转换为numpy数组\n",
    "        img = (img ).astype('uint8')  # 将图像转换为uint8类型\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(os.path.join(path, f\"{i}_{prefix}.png\"))\n",
    "\n",
    "# 保存all_images和all_brain_recons中的图像\n",
    "save_images(all_images, 'test', save_path)\n",
    "save_images(all_brain_recons, 'recon', save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum:\", all_brain_recons.min().item())\n",
    "print(\"Maximum:\", all_brain_recons.max().item())\n",
    "print(\"Mean:\", all_brain_recons.to(dtype=torch.float32).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e724d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axs = plt.subplots(ncols=200, squeeze=False,figsize=(64, 64))\n",
    "# # plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "# # fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "# for i in range(200):\n",
    "#     img = all_images[i].detach()\n",
    "#     img = transforms.ToPILImage()(img/255.0)\n",
    "#     axs[0, i].imshow(np.asarray(img))\n",
    "#     axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "\n",
    "# for i in range(200):\n",
    "#     img = all_brain_recons[i].detach()\n",
    "#     print(img)\n",
    "#     img = transforms.ToPILImage()(img/255.0)\n",
    "#     axs[1, i].imshow(np.asarray(img))\n",
    "#     axs[1, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "# # plt.subplots_adjust(wspace=0.1, hspace=0.001)\n",
    "# # fig.tight_layout()\n",
    "# # plt.subplots_adjust(wspace =0, hspace =0)#调整子图间距\n",
    "# plt.savefig('Reconstruction.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9b290-4016-4d33-823b-e0624a68b700",
   "metadata": {},
   "source": [
    "# Display reconstructions next to ground truth images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_interleaved = all_interleaved.transpose(1, 3)\n",
    "# all_interleaved.shape\n",
    "# # all_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Assuming all_interleaved is a tensor with shape [N, C, H, W]\n",
    "# where N is the number of images, C is the number of channels,\n",
    "# H is the height, and W is the width of the images.\n",
    "\n",
    "# Function to show a single image from all_interleaved\n",
    "def show_single_image(img_tensor, figsize=(5, 5)):\n",
    "    img_tensor = img_tensor.detach()\n",
    "    # Convert the tensor to PIL image\n",
    "    img = transforms.ToPILImage()(img_tensor/255.0)\n",
    "    # print(img)\n",
    "    # Plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(np.asarray(img))\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "# Show the first image from all_interleaved\n",
    "# show_single_image(all_interleaved[0, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88065b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_interleaved[0].to(device) - all_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db613b6a-33f1-459c-b494-52328297b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "all_images = transforms.Resize((imsize,imsize))(all_images)\n",
    "all_brain_recons = transforms.Resize((imsize,imsize))(all_brain_recons)\n",
    "# np.random.seed(0)\n",
    "ind = np.flip(np.array([i for i in range(len(split_idx))]))\n",
    "print(ind)\n",
    "\n",
    "all_interleaved = torch.zeros(len(ind)*2,3,imsize,imsize).to(device)\n",
    "\n",
    "icount = 0\n",
    "for t in ind:\n",
    "    all_interleaved[icount] = all_images[t].float().to(device)\n",
    "    print(\"all_interleaved\", all_interleaved[0])\n",
    "    all_interleaved[icount+1] = all_brain_recons[t].float().to(device)\n",
    "    icount += 2\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs,figsize):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        print(i)\n",
    "        img = img.detach()\n",
    "        img = transforms.ToPILImage()(img/255.0)\n",
    "        print(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        plt.show()\n",
    "    \n",
    "grid = make_grid(all_interleaved, nrow=10, padding=2)\n",
    "# print(grid)\n",
    "show(grid,figsize=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torchvision.utils import make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Assume all_images and all_brain_recons are defined and are lists or datasets of images\n",
    "# batch_size = 10  # Adjust batch_size depending on your GPU memory\n",
    "# num_batches = int(np.ceil(len(ind) / batch_size))\n",
    "# ind = np.flip(np.array([112,119,101,44,159,22,173,174,175,189]))\n",
    "# for batch_idx in range(num_batches):\n",
    "#     batch_start = batch_idx * batch_size\n",
    "#     batch_end = min(batch_start + batch_size, len(ind))\n",
    "#     batch_indices = ind[batch_start:batch_end]\n",
    "\n",
    "#     all_interleaved = torch.zeros(len(batch_indices)*2, 3, imsize, imsize)\n",
    "\n",
    "#     icount = 0\n",
    "#     for t in batch_indices:\n",
    "#         img = transforms.Resize((imsize, imsize))(all_images[t])\n",
    "#         recon = transforms.Resize((imsize, imsize))(all_brain_recons[t])\n",
    "\n",
    "#         all_interleaved[icount] = img\n",
    "#         all_interleaved[icount + 1] = recon\n",
    "#         icount += 2\n",
    "\n",
    "#     # Show or save the processed images here\n",
    "#     grid = make_grid(all_interleaved, nrow=10, padding=2)\n",
    "#     show(grid, figsize=(20, 16))\n",
    "\n",
    "#     # Optional: Clear cache if you are still facing memory issues\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ba39c-cd18-4942-a86a-ee640865bcdc",
   "metadata": {},
   "source": [
    "# 2-Way Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8056e-a80c-4abc-8c2d-3193bb43f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_way_identification(all_brain_recons, all_images, model, preprocess, feature_layer=None, return_avg=True):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_brain_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee0d96",
   "metadata": {},
   "source": [
    "# 50WayTop1 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
    "@torch.no_grad()\n",
    "def n_way_top_k_acc(pred, class_id, n_way, num_trials=40, top_k=1):\n",
    "    pick_range =[i for i in np.arange(len(pred)) if i != class_id]\n",
    "    acc_list = []\n",
    "    for t in range(num_trials):\n",
    "        idxs_picked = np.random.choice(pick_range, n_way-1, replace=False)\n",
    "        pred_picked = torch.cat([pred[class_id].unsqueeze(0), pred[idxs_picked]])\n",
    "        acc = accuracy(pred_picked.unsqueeze(0), torch.tensor([0], device=pred.device), \n",
    "                    top_k=top_k)\n",
    "        acc_list.append(acc.item())\n",
    "    return np.mean(acc_list), np.std(acc_list)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_n_way_top_k_acc(pred_imgs, ground_truth, n_way, num_trials, top_k, device, return_std=False):\n",
    "    weights = ViT_H_14_Weights.DEFAULT\n",
    "    model = vit_h_14(weights=weights)\n",
    "    preprocess = weights.transforms()\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    \n",
    "    acc_list = []\n",
    "    std_list = []\n",
    "    for pred, gt in zip(pred_imgs, ground_truth):\n",
    "        # pred = preprocess(Image.fromarray(pred.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        # gt = preprocess(Image.fromarray(gt.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        pred = preprocess(pred).unsqueeze(0).to(device)\n",
    "        gt = preprocess(gt).unsqueeze(0).to(device)\n",
    "        gt_class_id = model(gt).squeeze(0).softmax(0).argmax().item()\n",
    "        pred_out = model(pred).squeeze(0).softmax(0).detach()\n",
    "\n",
    "        acc, std = n_way_top_k_acc(pred_out, gt_class_id, n_way, num_trials, top_k)\n",
    "        acc_list.append(acc)\n",
    "        std_list.append(std)\n",
    "       \n",
    "    if return_std:\n",
    "        return acc_list, std_list\n",
    "    return acc_list\n",
    "with torch.no_grad():\n",
    "    result = get_n_way_top_k_acc(all_images, all_brain_recons, 50, 10, 1, device, return_std=False)\n",
    "result\n",
    "np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义图像尺寸\n",
    "imsize = 256\n",
    "# 调整图片尺寸\n",
    "all_images = transforms.Resize((imsize, imsize))(all_images)\n",
    "all_brain_recons = transforms.Resize((imsize, imsize))(all_brain_recons)\n",
    "\n",
    "# 筛选result中大于0.5的索引\n",
    "indices = [i for i, val in enumerate(result) if val > 0.5]\n",
    "print(\"Indices with result > 0.5:\", indices)\n",
    "\n",
    "# 初始化交错张量\n",
    "all_interleaved = torch.zeros(len(indices) * 2, 3, imsize, imsize).to(device)\n",
    "icount = 0\n",
    "\n",
    "for t in indices:\n",
    "    all_interleaved[icount] = all_images[t].float().to(device)\n",
    "    print(\"all_interleaved\", all_interleaved[0])\n",
    "    all_interleaved[icount + 1] = all_brain_recons[t].float().to(device)\n",
    "    icount += 2\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs, figsize):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        print(i)\n",
    "        img = img.detach()\n",
    "        img = transforms.ToPILImage()(img / 255.0)\n",
    "        print(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "grid = make_grid(all_interleaved, nrow=10, padding=2)\n",
    "show(grid, figsize=(64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义图像尺寸\n",
    "imsize = 256\n",
    "# 调整图片尺寸\n",
    "all_images = transforms.Resize((imsize, imsize))(all_images)\n",
    "all_brain_recons = transforms.Resize((imsize, imsize))(all_brain_recons)\n",
    "\n",
    "# 筛选result中大于0.5的索引\n",
    "indices = [i for i, val in enumerate(result) if val < 0.5]\n",
    "print(\"Indices with result < 0.5:\", indices)\n",
    "\n",
    "# 初始化交错张量\n",
    "all_interleaved = torch.zeros(len(indices) * 2, 3, imsize, imsize).to(device)\n",
    "icount = 0\n",
    "\n",
    "for t in indices:\n",
    "    all_interleaved[icount] = all_images[t].float().to(device)\n",
    "    print(\"all_interleaved\", all_interleaved[0])\n",
    "    all_interleaved[icount + 1] = all_brain_recons[t].float().to(device)\n",
    "    icount += 2\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs, figsize):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        print(i)\n",
    "        img = img.detach()\n",
    "        img = transforms.ToPILImage()(img / 255.0)\n",
    "        print(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "grid = make_grid(all_interleaved, nrow=10, padding=2)\n",
    "show(grid, figsize=(64, 64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63801703-0f78-47ca-89c6-54dcf71156a4",
   "metadata": {},
   "source": [
    "## PixCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89db923-16c8-4ec8-9bf9-cc9749031188",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((425, 425),interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "# Flatten images while keeping the batch dimension\n",
    "all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).cpu()\n",
    "all_brain_recons_flattened = preprocess(all_brain_recons).reshape(len(all_brain_recons), -1).cpu()\n",
    "\n",
    "print(all_images_flattened.shape)\n",
    "print(all_brain_recons_flattened.shape)\n",
    "\n",
    "corrsum = 0\n",
    "for i in tqdm(range(len(all_images_flattened))):\n",
    "    corrsum += np.corrcoef(all_images_flattened[i].numpy(), all_brain_recons_flattened[i].numpy())[0][1]\n",
    "corrmean = corrsum / len(all_images_flattened)\n",
    "\n",
    "pixcorr = corrmean\n",
    "print(pixcorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740d15b-1a02-4d50-9fe4-9dbfae38800e",
   "metadata": {},
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712ab76-1c8f-4232-995c-d06f8191d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/zijin-gu/meshconv-decoding/issues/3\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])\n",
    "\n",
    "# convert image to grayscale with rgb2grey\n",
    "img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu())\n",
    "recon_gray = rgb2gray(preprocess(all_brain_recons).permute((0,2,3,1)).cpu())\n",
    "print(\"converted, now calculating ssim...\")\n",
    "\n",
    "ssim_score=[]\n",
    "for im,rec in tqdm(zip(img_gray,recon_gray),total=len(all_images)):\n",
    "    ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "\n",
    "ssim = np.mean(ssim_score)\n",
    "print(ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4554b22-7faa-4e59-a6db-83bf775dd9e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc906db-bc5f-4cce-87f4-878706c14d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4','features.11']).to(device)\n",
    "alex_model.eval().requires_grad_(False)\n",
    "\n",
    "# see alex_weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Lambda(lambda x: x.float()/255),    \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Ensure all_images and all_brain_recons are tensors on the correct device and are floating-point\n",
    "all_images = all_images.to(device).float()  # Ensure conversion to float\n",
    "all_brain_recons = all_brain_recons.to(device).float()  # Ensure conversion to float\n",
    "\n",
    "layer = 'early, AlexNet(2)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.4')\n",
    "alexnet2 = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {alexnet2:.4f}\")\n",
    "\n",
    "layer = 'mid, AlexNet(5)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.11')\n",
    "alexnet5 = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {alexnet5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8ac02-a024-442f-a0a9-638a8afdeb0d",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b088c-07b4-4164-9e38-bdb5d43d58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception_model = create_feature_extractor(inception_v3(weights=weights), \n",
    "                                           return_nodes=['avgpool']).to(device)\n",
    "inception_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Lambda(lambda x: x.float()/255.0),    \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])# Ensure all_images and all_brain_recons are tensors on the correct device and are floating-point\n",
    "all_images = all_images.to(device).float()  # Ensure conversion to float\n",
    "all_brain_recons = all_brain_recons.to(device).float()  # Ensure conversion to float\n",
    "\n",
    "all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                        inception_model, preprocess, 'avgpool')\n",
    "        \n",
    "inception = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {inception:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12537959-3045-46f2-833f-80ff6327d40a",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc928ed-952c-4b2f-ae05-2cd8b8c770c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Lambda(lambda x: x.float()/255.0),    \n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "# Ensure all_images and all_brain_recons are tensors on the correct device and are floating-point\n",
    "all_images = all_images.to(device).float()  # Ensure conversion to float\n",
    "all_brain_recons = all_brain_recons.to(device).float()  # Ensure conversion to float\n",
    "\n",
    "all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                        clip_model.encode_image, preprocess, None) # final layer\n",
    "clip_ = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {clip_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a5a08-8acb-4fd9-b581-697df7b845db",
   "metadata": {},
   "source": [
    "### Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103b5af-ae7c-4f07-94e0-6b35d951ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "weights = EfficientNet_B1_Weights.DEFAULT\n",
    "eff_model = create_feature_extractor(efficientnet_b1(weights=weights), \n",
    "                                    return_nodes=['avgpool']).to(device)\n",
    "eff_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Process all_images\n",
    "gt = eff_model(torch.stack([preprocess(img.float()) for img in all_images.to(device)], dim=0))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "\n",
    "# Process all_brain_recons\n",
    "fake = eff_model(torch.stack([preprocess(recon.float()) for recon in all_brain_recons.to(device)], dim=0))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "effnet = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "print(\"Distance:\",effnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c764bd-4c46-4102-9342-5d63bc47c4e4",
   "metadata": {},
   "source": [
    "### SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all_images and all_brain_recons are tensors on the correct device and are floating-point\n",
    "all_images = all_images.to(device).float()  # Ensure conversion to float\n",
    "all_brain_recons = all_brain_recons.to(device).float()  # Ensure conversion to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacb33d-06b7-4847-b106-3a8ae15798f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "swav_model = create_feature_extractor(swav_model, \n",
    "                                    return_nodes=['avgpool']).to(device)\n",
    "swav_model.eval().requires_grad_(False)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Lambda(lambda x: x.float()/255.0),    \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Ensure all_images and all_brain_recons are tensors on the correct device and are floating-point\n",
    "all_images = all_images.to(device).float()  # Ensure conversion to float\n",
    "all_brain_recons = all_brain_recons.to(device).float()  # Ensure conversion to float\n",
    "\n",
    "gt = swav_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = swav_model(preprocess(all_brain_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "swav = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "print(\"Distance:\",swav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8674ca-b936-41c0-bba3-ec87a94a5deb",
   "metadata": {},
   "source": [
    "# Display in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbee6a-56fc-4bd7-9d52-4413c7a01a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store variable names and their corresponding values\n",
    "data = {\n",
    "    \"Metric\": [\"PixCorr\", \"SSIM\", \"AlexNet(2)\", \"AlexNet(5)\", \"InceptionV3\", \"CLIP\", \"EffNet-B\", \"SwAV\", \"Classification\"],\n",
    "    \"Value\": [pixcorr, ssim, alexnet2, alexnet5, inception, clip_, effnet, swav, np.mean(result)],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    # save table to txt file\n",
    "    df.to_csv(f'{recon_path[:-3]}.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd5892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
